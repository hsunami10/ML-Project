<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>EECS 349 Final Project</title>
    <link rel="stylesheet" href="styles.css">
  </head>

  <body>
    <h1>Deep Q-Learning Game AI</h1>
    <h3>John Laboe (<i>johnlaboe2020@u.northwestern.edu</i>) and Michael Hsu (<i>michaelhsu2020@u.northwestern.edu</i>)</h3>
    <h4>EECS 349: Machine Learning, Northwestern University</h4>
    
    <canvas id="canvas"></canvas>

    <div class="section" id="synopsis">
      <p class="section-title">Synopsis</p>
      <p class="ptext">The goal of this project was to create our own game, then create a deep-learning AI that could play it. Most other Reinforcement Learning Game AI projects on the internet tackle simpler games and the only input they have to their network is an image of the game. In our scenario, since we have built our own game, we get access to variables such as speed, player position, player direction, projectile position, without the need for image preprocessing or use of convolutional neural network layers. Further, it is interesting to see how the AI learning players adapt relative to how human players would adapt.</p>
      <p class="ptext">In terms of implementation, we used a neural network to implement a Q-learning algorithm. The features to the learner are information from the game such as player position, direction, velocity, shield and power as well as the position and velocity of projectiles. </p>
      <p class="ptext">While some results show that we implemented Deep Q-learning successfully, overall our AI did not learn to play very well. The AI does not seem to play like a human. Many of its moves seem erratic, and it can't seem to put together strings of actions that will earn high reward.</p>
    </div>
    <div class="section" id="implementation">
      <p class="section-title">Implementation</p>
      <p class="subsection-title">Game Implementation and Description</p>
      <p class="ptext">The goal with creating our own game was to give a complex task for our AI to learn. The objective of the game is simple: lower your opponents health points to 0 or have more HP when the game ends. The player can jump, and move left and right. The way in which a player lowers his enemy's HP is by shooting projectiles at the opponent. The size and damage of the projectile depends on how much energy (shown as red in-game) the player has. A player may defend itself against projectiles by dodging with movement or using a shield. The shield also has its own energy bar, and the user may hold the shield as long it has the energy. If the shield energy is at 0, then HP loss is proportional to the time the shield is held. Shield energy also drops proportional to the damage of the projectile it blocks.</p>
      <p class="subsection-title">Feature Selection</p>
      <p>One very important part of training our AI is choosing which features to feed it. There are a few stages to this. The first of which is choosing which information to actually extract from the game at each frame. For each player, we record its position (x and y), velocity (x and y), health, projectile energy, shield energy, and finally whether or not its shield is equipped. Then for each player, we record values for it's two oldest projectiles: the x and y position, the velocity, and the size. With two players, and two projectiles per player, we are recording 32 values at each frame.</p>
      <p>In researching other Q-learning game AI's, we discovered that many stack multiple frames together in order to train a network to make decision on data which corresponds to multiple time stamps. In order to delay making this decision, we allowed this to be an input to building our neural network. If you look at the settings of the game, you see we give the option of either stacking 1, 2, 3, or 4 frames, which means inputs of size 32, 64, 96, and 128 respectively to our network.</p>
      <p>Another common adjustment to the inputs of the neural network is to not consider every frame, but instead to skip frames, and only consider every third or fourth frame. We also give the user the option to set this parameter for the network, with the option of taking every frame, every 2nd frame, every 3rd frame or every 4th game. One of the reasons this method is used is because the the frame rate of the game high enough that consecutive frames do not yield enough useful information.</p>
      <p class="subsection-title">Deep Q-Learning Implementation</p>
      <p>There are a few parts to designing the learning algorithm. First is the implementation of the neural network. In Q-learning, there is a Q-value for each action A and for each state S. What this means is that there is some function Q = Q(S, A). We want our neural network to output a Q-value for each A, therefore our network is only a function of S, but has A outputs. We know the input size of our game state S (discussed in the first paragraph of the game implementation section), but we must also address the size of the action space A. Our game has five input actions: move left, move right, jump, shoot, and shield. Overall, this makes for 32 total actions (2^5).</p>
      <p>Next, we must decide on the inner structure of the network. We gave the user the option to set the number of hidden layers to 1, 2 or 3. Each of these inner layers was set to be an exponential linear units, while the output layer was just a regular linear unit.</p>
      <p>The optimization we perform over this network is to reduce the mean squared error between the expected Q-values for all outputs given a state and the predicted Q-values output by the network. Consequently, we must determine what is the expected Q-value for each action A given state S. The solution is to use the predicted values as the expected values, but to update the Q-value of the action taking for the training example in question. For this, we say the expected value of this action is the reward of the example plus the maximum predicted Q-value of the next state s'. The formula for this is:</p>
      <p>E[Q(s,a)] = r + (DISCOUNT_RATE) * max(Q(s', A))</p>
      <p>Therefore, for each training example we need a state s, an action a, a reward r, and a state at time t+1, s'. We also train our network one example at a time.</p>
      <p>There are a few constants for the network that we have defined for our implementation. First is the learning rate, which we have set at 0.05. The second is the Discount Rate. The discount rate affects how much future actions reward propagates backward into earlier decisions. We have this set at 0.9. Finally, there is the exploration rate. This is important for the game AI so that it takes random decision every so often. We have it set to 0.02, which is to say that 2% of the actions taken by the AI are completely random.</p>
      <p>Lastly, we need to choose a reward function for our training. For us, we decided that the reward function was the loss in the enemies' health minus the loss of the current players' health.</p>
      <p class="subsection-title">Web Server Implementation</p>
      <p>The web server is important to our implementation so that anyone who plays games on the site contributes to the AI learning. At the conclusion of each game, each AI model is trained and then sent back to the web server to be saved. You can see the training time of each algorithm (training time corresponds to the amount of game time, not actual training time). When the user changes the parameter they want to set for the neural network, the network is loaded from the server. The server also holds the total training time of each network.</p>
    </div>

    <div class="section" id="analysis">
      <p class="section-title">Results and Analysis</p>
      <p>The first result of training that we saw happened very early in the training. Considering the game mechanic that causes health to deplete if the shield is active and the shield energy is at 0, most of the networks we trained learned very quickly to not use that shield. Of course, with 32 actions, and half of them having the button for equipping the shield pressed, the habit sometimes came back, before being quickly erased again. The second macro action which some of our networks somewhat learned was to jump to avoid the projectiles. While the AI is not always very consistent at jumping, it is clear for a few of the AI that multiple dodges in a row are not merely luck but actually correlated attempts to dodge the projectiles.</p>
      <p>As demonstration of these results is best seen using the network we trained the most (Network_4_4_3, which corresponds to 3 layers, 4 frames stacked, and 4 frame skip). If you want to check this out, make sure you set both of the players to AI; don't leave them on the human setting or you'll have to play.</p>
      <p>Alternatively, you could play as the human against the AI and probably get an easy victory. For that reason, we need to dive in to the analysis of the shortcomings of our system.</p>
      <p>There are many shortcomings to this system, which set up grounds for future work on this project. The first of which is the reward system. If you pay attention to the AI, you can see that it does not always shoot in the direction of the other player. The reason for this is that the reward for shooting a player is tied to the time that the enemy is actually hit with the projectile, not when it is shot. This means that the reward has to propagate backwards enough to influence that decision. However, with a propagation rate of 0.9, the propagation of the reward doesn't travel very far, especially when it can become convoluted with other rewards. Even if the propagation rate was high enough such that the reward did successfully propagate, then training time would be severely long because the time of propagation of reward over training would be large.</p>
      <p>Another major issue with our input features is how many frames we skip relative to our frames per second. In the right hand corner of the game screen, you can probably see the game getting 50-60 frames per second. What this means, best case when we are only analyze every fourth frame, is that we get about 10 frames per second. With a Discount Rate of 0.9, only 34% of the reward from time t is theoretically propagated to time t - 1 second. This is not terrible, but if we want our AI to make intelligent decision based on future rewards, then we probably want this reward to propagate a little stronger.</p>
      <p>Another potential concern for the system would be the exploration rate. Given about 50 frames per second and an exploration of 0.2, we are expected to make a random decision once per second. It's possible this could be interefering with our visual results.</p>
      <p>Finally, though it's hard to say for sure right now, there might be a bias with the starting position of a player. The network for a given player might be better suited to playing the left side or the right side.</p>
    </div>
    <div class="section" id="future">
      <p class="section-title">Future Work</p>
      <p>It is a little bit unfortunate that we ran out of time to act on the short comings of the system that we identified above, but the long training times make it difficult. Especially when we are trying to identify which input parameters perform the best for our system.</p>
      <p>On the shortcomings of the reward system, one change would be to give the player a reward for hitting his enemy at the time which the player shoots the projectile. This would require a hefty reworking of the code behind storing the rewards and the states, since right now the reward is just calculated by taking the HP levels at consecutive states and performing the necessary addition and subtraction. With the new system we would have to keep track of when projectiles were launched, then do a lookup for these times when the projectile hits the enemy. A natural question that arises out of this is whether or not the player should receive reward if the projectile hits the enemy but the enemy has its shield equipped. Another change would be to give reward to the player if it successfully blocks a projectile with a shield. This method for protection was rarely seen among the trained networks.</p>
      <p>In regards to the frame rate, the simple fix would be to add more options for the frame skip parameter</p>
      <p>To overcome the exploration rate problem, it would be best to start with a very small exploration rate, then manually increase it as the actions of the AI became more repetitive. Early on, there is enough adjustment to the system through training that each game seems entirely different, despite the AI starting in the same place each game.</p>
      <p>Lastly, unrelated to the algorithm itself, it would be nice to implement a record system which kept track of wins, losses and ties for each network/side. With this information, over many games played, we could determine which network is performing the best and if either starting side of the game has bias for each network.</p>
    </div>
    <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.3/dist/tf.min.js"> </script>
    <script src="DeepQNetwork.js" charset="utf-8"></script>
    <script src="Experience.js" charset="utf-8"></script>
    <script src ="FrameBuffer.js" charset="utf-8"></script>
    <script src="enums.js" charset="utf-8"></script>
    <script src="button.js" charset="utf-8"></script>
    <script src="projectile.js" charset="utf-8"></script>
    <script src="input.js" charset="utf-8"></script>
    <script src="player.js" charset="utf-8"></script>
    <script src="game.js" charset="utf-8"></script>
    <script src="main.js" charset="utf-8"></script>
  </body>
</html>
