<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>EECS 349 Final Project</title>
    <link rel="stylesheet" href="styles.css">
  </head>

  <body>
    <h1>Deep Q-Learning Game AI</h1>
    <h3>John Laboe (<i>johnlaboe2020@u.northwestern.edu</i>) and Michael Hsu (<i>michaelhsu@u.northwestern.edu</i>)</h3>
    <h4>EECS 349: Machine Learning, Northwestern University</h4>
    
    <canvas id="canvas"></canvas>

    <div class="section" id="synopsis">
      <p class="section-title">Synopsis</p>
      <p class="ptext">The goal of this project was to create our own game, then create a deep-learning AI that could play it. Most other Reinforcement Learning Game AI projects on the internet tackle simpler games and the only input they have to their network is an image of the game. In our scenario, since we have built our own game, we get access to variables such as speed, player position, player direction, projectile position, without the need for image preprocessing or use of convolutional neural network layers. Further, it is interesting to see how the AI learning players adapt relative to how human players would adapt.</p>
      <p class="ptext">In terms of implementation, we used a neural network to implement a Q-learning algorithm. The features to the learner are information from the game such as player position, direction, velocity, shield and power as well as the position and velocity of projectiles. </p>
      <p class="ptext">While some results show that we implemented Deep Q-learning successfully, overall our AI did not learn to play very well. The AI does not seem to play like a human. Many of its moves seem erratic, and it can't seem to put together strings of actions that will earn high reward.</p>
    </div>
    <div class="section" id="implementation">
      <p class="section-title">Implementation</p>
      <p class="subsection-title">Game Implementation and Description</p>
      <p class="ptext">The goal with creating our own game was to give a complex task for our AI to learn. The objective of the game is simple: lower your opponents health points to 0 or have more HP when the game ends. The player can jump, and move left and right. The way in which a player lowers his enemy's HP is by shooting projectiles at the opponent. The size and damage of the projectile depends on how much energy (shown as red in-game) the player has. A player may defend itself against projectiles by dodging with movement or using a shield. The shield also has its own energy bar, and the user may hold the shield as long it has the energy. If the shield energy is at 0, then HP loss is proportional to the time the shield is held. Shield energy also drops proportional to the damage of the projectile it blocks.</p>
      <p class="subsection-title">Feature Selection</p>
      <p>One very important part of training our AI is choosing which features to feed it. There are a few stages to this. The first of which is choosing which information to actually extract from the game at each frame. For each player, we record its position (x and y), velocity (x and y), health, projectile energy, shield energy, and finally whether or not its shield is equipped. Then for each player, we record values for it's two oldest projectiles: the x and y position, the velocity, and the size. With two players, and two projectiles per player, we are recording 32 values at each frame.</p>
      <p>In researching other Q-learning game AI's, we discovered that many stack multiple frames together in order to train a network to make decision on data which corresponds to multiple time stamps. In order to delay making this decision, we allowed this to be an input to building our neural network. If you look at the settings of the game, you see we give the option of either stacking 1, 2, 3, or 4 frames, which means inputs of size 32, 64, 96, and 128 respectively to our network.</p>
      <p>Another common adjustment to the inputs of the neural network is to not consider every frame, but instead to skip frames, and only consider every third or fourth frame. We also give the user the option to set this parameter for the network, with the option of taking every frame, every 2nd frame, every 3rd frame or every 4th game. One of the reasons this method is used is because the the frame rate of the game high enough that consecutive frames do not yield enough useful information.</p>
      <p class="subection-title">Deep Q-Learning Implementation</p>
      <p>There are a few parts to designing the learning algorithm. First is the implementation of the neural network. In Q-learning, there is a Q-value for each action A and for each state S. What this means is that there is some function Q = Q(S, A). We want our neural network to output a Q-value for each A, therefore our network is only a function of S, but has A outputs. We know the input size of our game state S (discussed in the first paragraph of the game implementation section), but we must also address the size of the action space A. Our game has five input actions: move left, move right, jump, shoot, and shield. Overall, this makes for 32 total actions (2^5).</p>
    </div>
    <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.3/dist/tf.min.js"> </script>
    <script src="DeepQNetwork.js" charset="utf-8"></script>
    <script src="Experience.js" charset="utf-8"></script>
    <script src ="FrameBuffer.js" charset="utf-8"></script>
    <script src="enums.js" charset="utf-8"></script>
    <script src="button.js" charset="utf-8"></script>
    <script src="projectile.js" charset="utf-8"></script>
    <script src="input.js" charset="utf-8"></script>
    <script src="player.js" charset="utf-8"></script>
    <script src="game.js" charset="utf-8"></script>
    <script src="main.js" charset="utf-8"></script>
  </body>
</html>
