<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>EECS 349 Final Project</title>
    <link rel="stylesheet" href="styles.css">
  </head>

  <body>
    <div id="head">
      <h1>EECS 349 Machine Learning, Northwestern University: Deep Q-Learning Game AI</h1>
      <h3>John Laboe (<i>johnlaboe2020@u.northwestern.edu</i>) and Michael Hsu (<i>michaelhsu2020@u.northwestern.edu</i>)</h3>
      <h3>Project Github: <a href="https://github.com/hsunami10/ML-Project">Take a look at our code!</a></h3>
      
    </div>

    <canvas id="canvas"></canvas>
    <p class="section-title">Controls</p>
    <div id="control">
      <div class="controls_img">
        <img src="player1_keys.png" class="controls">
      </div>

      <div class="controls_img">
        <img src="player2_keys.png" class="controls">
      </div>
    </div>

      
    <div class="section" id="synopsis">
      <p class="section-title">Synopsis</p>
      <p class="ptext">The goal of this project was to create our own game, then create a deep-learning AI that could play it. Most other Reinforcement Learning Game AI projects on the internet tackle simpler games and the only input they have to their network is an image of the game. In our scenario, since we have built our own game, we get access to variables such as speed, player position, player direction, projectile position, without the need for image preprocessing or use of convolutional neural network layers. Further, it is interesting to see how the AI learning players adapt relative to how human players would adapt.</p>
      <p class="ptext">In terms of implementation, we used a neural network to implement a Q-learning algorithm. The features to the learner are information from the game such as player position, direction, velocity, shield and power as well as the position and velocity of projectiles. </p>
      <p class="ptext">While some results show that we implemented Deep Q-learning algorithm successfully, overall our AI did not learn to play very well. The AI does not seem to play like a human. Many of its moves seem erratic, and it can't seem to put together strings of actions that will earn high reward.</p>
    </div>
    <div class="section" id="implementation">
      <p class="section-title">Implementation</p>
      <p class="subsection-title">Game Implementation and Description</p>
      <p class="ptext">The goal with creating our own game was to give a complex task for our AI to learn. The objective of the game is simple: lower your opponents health points to 0 or have more HP when the game ends. The player can jump, and move left and right. The way in which a player lowers his enemy's HP is by shooting projectiles at the opponent. The size and damage of the projectile depends on how much energy (shown as red in-game) the player has. A player may defend itself against projectiles by dodging with movement or using a shield. The shield also has its own energy bar, and the user may hold the shield as long it has the energy. If the shield energy is at 0, then HP loss is proportional to the time the shield is held. Shield energy also drops proportional to the damage of the projectile it blocks.</p>
      <p class="subsection-title">Feature Selection</p>
      <p class="ptext">One very important part of training our AI is choosing which features to feed it. There are a few stages to this. The first of which is choosing which information to actually extract from the game at each frame. For each player, we record its position (x and y), velocity (x and y), health, projectile energy, shield energy, and finally whether or not its shield is equipped. Then for each player, we record values for it's two oldest projectiles: the x and y position, the velocity, and the size. With two players, and two projectiles per player, we are recording 32 values at each frame.</p>
      <p class="ptext">In researching other Q-learning game AI's, we discovered that many stack multiple frames together in order to train a network to make decision on data which corresponds to multiple time stamps. In order to delay making this decision, we allowed this to be an input to building our neural network. If you look at the settings of the game, you see we give the option of either stacking 1, 2, 3, or 4 frames, which means inputs of size 32, 64, 96, and 128 respectively to our network.</p>
      <p class="ptext">Another common adjustment to the inputs of the neural network is to not consider every frame, but instead to skip frames, and only consider every third or fourth frame. We also give the user the option to set this parameter for the network, with the option of taking every frame, every 2nd frame, every 3rd frame or every 4th game. One of the reasons this method is used is because the the frame rate of the game high enough that consecutive frames do not yield enough useful information.</p>
      <p class="subsection-title">Deep Q-Learning Implementation</p>
      <p class="ptext">There are a few parts to designing the learning algorithm. First is the implementation of the neural network. In Q-learning, there is a Q-value for each action A and for each state S. What this means is that there is some function Q = Q(S, A). We want our neural network to output a Q-value for each A, therefore our network is only a function of S, but has A outputs. We know the input size of our game state S (discussed in the first paragraph of the game implementation section), but we must also address the size of the action space A. Our game has five input actions: move left, move right, jump, shoot, and shield. Overall, this makes for 32 total actions (2^5).</p>
      <p class="ptext">Next, we must decide on the inner structure of the network. We gave the user the option to set the number of hidden layers to 1, 2 or 3. Each of these inner layers was set to be an exponential linear units, while the output layer was just a regular linear unit.</p>
      <p class="ptext">The optimization we perform over this network is to reduce the mean squared error between the expected Q-values for all outputs given a state and the predicted Q-values output by the network. Consequently, we must determine what is the expected Q-value for each action A given state S. The solution is to use the predicted values as the expected values, but to update the Q-value of the action taking for the training example in question. For this, we say the expected value of this action is the reward of the example plus the maximum predicted Q-value of the next state s'. The formula for this is:</p>
      <p style="text-align: center;" class="equation"><i>E[Q(s,a)] = r + (DISCOUNT_RATE) * max(Q(s', A))</i></p>
      <p class="ptext">Therefore, for each training example we need a state s, an action a, a reward r, and a state at time t+1, s'. We also train our network one example at a time.</p>
      <p class="ptext">There are a few constants for the network that we have defined for our implementation. First is the learning rate, which we have set at 0.0005. The second is the Discount Rate. The discount rate affects how much future actions reward propagates backward into earlier decisions. We have this set at 0.95. Finally, there is the exploration rate. This is important for the game AI so that it takes random decision every so often. We have it set to 0.015, which is to say that 1.5% of the actions taken by the AI are completely random.</p>
      <p class="ptext">Lastly, we need to choose a reward function for our training. For us, we decided that the reward function was the change in the player’s health (loss of health means negative reward) plus rewards for each projectile that hits the opponent. When the projectile hits the opponent, we map the reward back to the frame during which the projectile was shot. </p>
      <p class="subsection-title">Web Server Implementation</p>
      <p class="ptext">The web server is important to our implementation so that anyone who plays games on the site contributes to the AI learning. At the conclusion of each game, each AI model is trained and then sent back to the web server to be saved. You can see the training time of each algorithm (training time corresponds to the amount of game time, not actual training time). When the user changes the parameter they want to set for the neural network, the network is loaded from the server. The server also holds the total training time of each network.</p>
    </div>

    <div class="section" id="analysis">
      <p class="section-title">Results and Analysis</p>
      <p class="ptext">The first result of training that we saw happened very early in the training. Considering the game mechanic that causes health to deplete if the shield is active and the shield energy is at 0, most of the networks we trained learned very quickly to not use that shield. The second thing that the AI learned is that when you are directly next to your opponent, you will get a high reward when shooting because it will always hit. As a consequence, the AI often gets stuck in corners (when playing against other AI). We also see the AI still use the shield, just more sporadically. In this way it finds a way to use a shield without losing health, though sometimes the reward from blocking projectiles increases dependency on shields again, forcing the network to retrain itself not to use shield so often. Our AI also likes to jump a lot. We also see that the behavior of the AI changes a lot after training, even though we set a low training rate. This is probably due to the fact that we train one example at a time instead of in batches.</p>
      <p class="ptext">A demonstration of these results is best seen using the network Network_2_4_1, which corresponds to 1 layers, 2 frames stacked, and 4 frame skip. If you want to check this out, make sure you set both of the players to AI; don't leave them on the human setting or you'll have to play. We found that we don’t need the stacked frames as much as other learners do because speed is part of our input features so change over time of the system is already a feature unlike other systems which captured the pixels of the game.</p>
      <p class="ptext">Alternatively, you could play as the human against the AI and probably get an easy victory. For that reason, we need to dive in to the analysis of the shortcomings of our system.</p>
      <p class="ptext">There are many shortcomings to this system, which set up grounds for future work on this project. The first of which is the reward system. A major issue with our input features is how many frames we skip relative to our frames per second. In the right hand corner of the game screen, you can probably see the game getting 50-60 frames per second. What this means, best case when we are only analyze every fourth frame, is that we get about 10 frames per second. This means the reward for particular decisions doesn’t propagate backwards in time as far. Assume propagation is the Discount Rate ^ # of frames backwards. A higher discount rate helps propagate these rewards farther back in the frames and therefore farther back in time. An increase in frames per second equals a decrease in propagation as a function of time.</p>
      <p class="ptext">Another potential concern for the system would be the exploration rate. Given about 50 frames per second and an exploration of 0.015, we are expected to make a random decision once per 1.3 seconds. It's possible this could be interfering with our visual results.</p>
      <p class="ptext">Finally, though it's hard to say for sure right now, there might be a bias with the starting position of a player. The network for a given player might be better suited to playing the left side or the right side.</p>
    </div>
    <div class="section" id="future">
      <p class="section-title">Future Work</p>
      <p class="ptext">It is a little bit unfortunate that we ran out of time to act on the short comings of the system that we identified above, but the long training times make it difficult. Especially when we are trying to identify which input parameters perform the best for our system.</p>
      <p class="ptext">On the shortcomings of the reward system, one change would be to change the amount of reward for losing health or hitting an opponent with a projectile. In this way, we could help the AI prioritize shooting or saving health. We also could give the AI a reward for blocking a projectile with a shield, though given our current results, we probably want less preference towards shield. Alternatively, we could batch train or lower the training rate so that we do not have such drastic changes in our network after training.</p>
      <p class="ptext">In regards to the frame rate, the simple fix would be to add more options for the frame skip parameter</p>
      <p class="ptext">To overcome the exploration rate problem, it would be best to start with a very small exploration rate, then manually increase it as the actions of the AI became more repetitive. Early on, there is enough adjustment to the system through training that each game seems entirely different, despite the AI starting in the same place each game.</p>
      <p class="ptext">Lastly, unrelated to the algorithm itself, it would be nice to implement a record system which kept track of wins, losses and ties for each network/side. With this information, over many games played, we could determine which network is performing the best and if either starting side of the game has bias for each network.</p>
    </div>
    <div class="section" id="tasks">
      <p class="section-title">Delegation of Tasks</p>
      <p class="ptext">
        The basic user controls, graphics, and menu interface were implemented by Michael. The physics of the game, shield animations, projectiles, and bars were implemented by John.
        <br>
        Both worked on using TensorFlow for the Deep Q-Learning model implementation, with John leading the effort.
        <br>
        Michael wrote the backend server, including all API endpoints, handling saving files (models, model weights, training times) to the server, and uploading these files when requested by the client.
      </p>
    </div>
    <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.13.3/dist/tf.min.js"> </script>
    <script src="DeepQNetwork.js" charset="utf-8"></script>
    <script src="Experience.js" charset="utf-8"></script>
    <script src ="FrameBuffer.js" charset="utf-8"></script>
    <script src="enums.js" charset="utf-8"></script>
    <script src="button.js" charset="utf-8"></script>
    <script src="projectile.js" charset="utf-8"></script>
    <script src="input.js" charset="utf-8"></script>
    <script src="player.js" charset="utf-8"></script>
    <script src="game.js" charset="utf-8"></script>
    <script src="main.js" charset="utf-8"></script>
  </body>
</html>
